{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ **Introduction**\n",
    "\n",
    "In this lab, we explore how to build and train an **Autoencoder for image generation** using **PyTorch**.  \n",
    "Our goal is to create a generative model that can learn to compress MNIST digit images into a compact latent representation and then reconstruct (or generate) images from these compressed encodings.\n",
    "\n",
    "#### ðŸŽ¨ **What is an Autoencoder?**\n",
    "An autoencoder is a neural network architecture designed to learn efficient data representations in an unsupervised manner. It consists of two main components:\n",
    "\n",
    "1. **Encoder** ðŸ”½ â€” Compresses input data into a lower-dimensional latent space (bottleneck)\n",
    "2. **Decoder** ðŸ”¼ â€” Reconstructs the original data from the compressed representation\n",
    "\n",
    "#### ðŸŽ¯ **Our Approach**\n",
    "1. **Training Phase:**\n",
    "   - Feed MNIST images (28Ã—28) through the encoder to get compact embeddings\n",
    "   - Pass embeddings through the decoder to reconstruct images\n",
    "   - Minimize reconstruction error (MSE Loss) between original and reconstructed images\n",
    "   \n",
    "2. **Generation Phase:**\n",
    "   - Sample random points in the latent space\n",
    "   - Feed them through the decoder to generate new digit images\n",
    "\n",
    "#### ðŸ”§ **Architecture Constraints**\n",
    "In this notebook, we'll use **only Linear (fully connected) layers and BatchNorm** â€” no CNNs, attention mechanisms, or other advanced techniques. This limitation means:\n",
    "- Performance may be moderate compared to CNN-based autoencoders\n",
    "- But it clearly demonstrates the core autoencoder concept!\n",
    "\n",
    "By the end of this lab, we'll see our model attempting to capture patterns in handwritten digits and generate new samples from random latent codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  **Importing Libraries and Frameworks**\n",
    "\n",
    "In this step, we import all the essential Python libraries for our autoencoder implementation:\n",
    "\n",
    "- **PyTorch** (`torch`) â€” core deep learning framework for tensor operations and automatic differentiation.  \n",
    "- **torch.nn** â€” neural network layers, activations, and loss functions.  \n",
    "- **NumPy** (`np`) â€” numerical operations and array handling.  \n",
    "- **torchvision** â€” dataset loading and image transformations.  \n",
    "  - `transforms` â€” for data augmentation and preprocessing\n",
    "  - `MNIST` â€” the handwritten digit dataset\n",
    "- **torch.utils.data.sampler** (`SubsetRandomSampler`) â€” for sampling subsets of data.  \n",
    "- **torchsummary** (`summary`) â€” provides detailed model architecture summaries.  \n",
    "- **Matplotlib** (`plt`) â€” for visualizing training curves and generated images.  \n",
    "- **mpl_toolkits.axes_grid1** (`ImageGrid`) â€” for creating organized grids of images.  \n",
    "- **IPython.display** (`clear_output`) â€” for cleaning console output.\n",
    "\n",
    "These imports provide everything we need to build, train, and visualize our autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1698566894922,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "zwfZGneCdQSg"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24297,
     "status": "ok",
     "timestamp": 1698566919216,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "a9FjTEzAdQ28",
    "outputId": "5c169975-f38c-45ff-8fd8-51c9458dadee"
   },
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchsummary\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2233,
     "status": "ok",
     "timestamp": 1698566924856,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "tQaUBH8lmnnr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“‹ **Lab Contents and Autoencoder Overview**\n",
    "\n",
    "In this notebook, we'll build a **number image generator using autoencoders** trained on the MNIST dataset.\n",
    "\n",
    "#### ðŸ—ï¸ **Architecture Overview**\n",
    "\n",
    "Our model consists of two interconnected parts:\n",
    "\n",
    "**1. Encoder (Compression Network) ðŸ”½**\n",
    "- Takes a 28Ã—28 digit image (784 pixels)\n",
    "- Passes it through multiple linear layers\n",
    "- Outputs a compact **latent embedding** of specified size (e.g., 128 dimensions)\n",
    "- **Goal:** Learn meaningful, compressed representations of digits\n",
    "\n",
    "**2. Decoder (Reconstruction Network) ðŸ”¼**\n",
    "- Takes the latent embedding\n",
    "- Passes it through multiple linear layers (mirror of encoder)\n",
    "- Outputs a reconstructed 28Ã—28 image\n",
    "- **Goal:** Recreate the original image from the compressed representation\n",
    "\n",
    "#### ðŸŽ“ **Training Process**\n",
    "\n",
    "**Loss Function:** Mean Squared Error (MSE)\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC2jmrtmiUm7"
   },
   "source": [
    "### ðŸ“Š **Loading MNIST Dataset with Data Augmentation**\n",
    "\n",
    "In this section, we prepare the MNIST dataset with custom transformations for training.\n",
    "\n",
    "#### ðŸ”„ **Data Augmentation Pipeline**\n",
    "```python\n",
    "transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=4),  # Random shifts/translations\n",
    "    transforms.ToTensor(),                  # Convert to tensor [0, 1]\n",
    "])\n",
    "```\n",
    "\n",
    "**Why Data Augmentation?**\n",
    "- **RandomCrop(28, padding=4):**\n",
    "  - Adds 4 pixels of padding around the 28Ã—28 image (making it 36Ã—36)\n",
    "  - Randomly crops back to 28Ã—28\n",
    "  - **Effect:** Introduces slight translations and position variations\n",
    "  - **Benefit:** Model learns position-invariant representations\n",
    "  \n",
    "- **ToTensor():**\n",
    "  - Converts PIL images to PyTorch tensors\n",
    "  - Normalizes pixel values from [0, 255] to [0.0, 1.0]\n",
    "  - Changes format to (C, H, W) â€” PyTorch's expected format\n",
    "\n",
    "#### ðŸ“¦ **DataLoader Configuration**\n",
    "- **Batch Size:** 64 samples per batch\n",
    "- **Shuffle:** True â€” randomizes order each epoch for better training\n",
    "- **Dataset:** 60,000 training images from MNIST\n",
    "\n",
    "Data augmentation helps the autoencoder learn more robust representations that work across slight variations in digit positioning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1698566936296,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "b83RYTZueJCW"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Define data augmentation pipeline\n",
    "train_transforms = transforms.Compose([\n",
    "                        transforms.RandomCrop(28, padding=4),  # Random translation\n",
    "                        transforms.ToTensor(),                  # Convert to tensor [0, 1]\n",
    "                    ])\n",
    "\n",
    "# Load MNIST training data with transformations\n",
    "train_data = MNIST(root='./datasets', train=True, download=True, transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—ï¸ **Building the Autoencoder Architecture**\n",
    "\n",
    "Now we define our `AutoEncoderGenerator` model with encoder and decoder components.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”½ **Encoder Architecture (Compression Network)**\n",
    "\n",
    "The encoder progressively compresses the 784-dimensional input into a compact latent space:\n",
    "```\n",
    "Input (784) â†’ Linear(1024) â†’ LeakyReLU â†’ BatchNorm\n",
    "           â†’ Linear(512)  â†’ LeakyReLU â†’ BatchNorm\n",
    "           â†’ Linear(256)  â†’ LeakyReLU\n",
    "           â†’ Linear(dim_z) â†’ Tanh â†’ Latent Embedding\n",
    "```\n",
    "\n",
    "**Layer Breakdown:**\n",
    "1. **784 â†’ 1024:** Expand first (allows learning complex patterns)\n",
    "2. **1024 â†’ 512:** Begin compression\n",
    "3. **512 â†’ 256:** Continue compression\n",
    "4. **256 â†’ dim_z:** Final bottleneck (e.g., 128 dimensions)\n",
    "\n",
    "**Activation Functions:**\n",
    "- **LeakyReLU:** Allows small negative gradients (prevents dying ReLU problem)\n",
    "- **Tanh (final):** Squashes embeddings to [-1, 1] range\n",
    "- **BatchNorm:** Normalizes activations for stable training\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¼ **Decoder Architecture (Reconstruction Network)**\n",
    "\n",
    "The decoder mirrors the encoder, expanding from latent space back to image:\n",
    "```\n",
    "Latent (dim_z) â†’ Linear(256)  â†’ LeakyReLU\n",
    "               â†’ Linear(512)  â†’ LeakyReLU\n",
    "               â†’ Linear(1024) â†’ LeakyReLU\n",
    "               â†’ Linear(784)  â†’ Sigmoid â†’ Output Image\n",
    "```\n",
    "\n",
    "**Layer Breakdown:**\n",
    "1. **dim_z â†’ 256:** Begin expansion from compact representation\n",
    "2. **256 â†’ 512:** Continue expansion\n",
    "3. **512 â†’ 1024:** Further expansion\n",
    "4. **1024 â†’ 784:** Final reconstruction (28Ã—28 = 784 pixels)\n",
    "\n",
    "**Activation Functions:**\n",
    "- **LeakyReLU:** Non-linear transformations during expansion\n",
    "- **Sigmoid (final):** Squashes output to [0, 1] (valid pixel range)\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”„ **Forward Pass Flow**\n",
    "```python\n",
    "Input Image (batch, 1, 28, 28)\n",
    "    â†“ Flatten\n",
    "(batch, 784)\n",
    "    â†“ Encoder\n",
    "Latent Embedding (batch, dim_z)\n",
    "    â†“ Decoder\n",
    "Reconstructed (batch, 784)\n",
    "    â†“ Reshape\n",
    "Output Image (batch, 1, 28, 28)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸŽ¯ **Key Design Choices**\n",
    "\n",
    "1. **Symmetric Architecture:** Decoder mirrors encoder (common in autoencoders)\n",
    "2. **Bottleneck:** The `dim_z` parameter controls compression level\n",
    "   - Smaller dim_z â†’ more compression â†’ harder to learn\n",
    "   - Larger dim_z â†’ less compression â†’ easier but less efficient\n",
    "3. **No Skip Connections:** Unlike U-Net, we force all information through the bottleneck\n",
    "4. **Linear Layers Only:** Demonstrates core concept without spatial convolutions\n",
    "\n",
    "This architecture forces the model to learn efficient, compressed representations of handwritten digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1698566976409,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "iEF-W-XZnB68"
   },
   "outputs": [],
   "source": [
    "class AutoEncoderGenerator(nn.Module):\n",
    "  \"\"\"\n",
    "  Autoencoder for MNIST digit compression and reconstruction.\n",
    "  \n",
    "  Architecture:\n",
    "  - Encoder: Compresses 784D image to dim_z latent space\n",
    "  - Decoder: Reconstructs 784D image from latent space\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, dim_z):\n",
    "    super().__init__()\n",
    "    \n",
    "    # ============ ENCODER (Compression Network) ============\n",
    "    self.Encoder = nn.Sequential(\n",
    "        \n",
    "        # Layer 1: 784 â†’ 1024 (expand first for pattern learning)\n",
    "        nn.Linear(28*28, 1024),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        \n",
    "        # Layer 2: 1024 â†’ 512 (begin compression)\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.BatchNorm1d(512),\n",
    "        \n",
    "        # Layer 3: 512 â†’ 256 (continue compression)\n",
    "        nn.Linear(512, 256),\n",
    "        nn.LeakyReLU(),\n",
    "        \n",
    "        # Layer 4: 256 â†’ dim_z (bottleneck/latent space)\n",
    "        nn.Linear(256, dim_z),\n",
    "        nn.Tanh(),  # Squash embeddings to [-1, 1]\n",
    "      )\n",
    "\n",
    "    # ============ DECODER (Reconstruction Network) ============\n",
    "    self.Decoder = nn.Sequential(\n",
    "        \n",
    "        # Layer 1: dim_z â†’ 256 (begin expansion)\n",
    "        nn.Linear(dim_z, 256),\n",
    "        nn.LeakyReLU(),\n",
    "        \n",
    "        # Layer 2: 256 â†’ 512 (continue expansion)\n",
    "        nn.Linear(256, 512),\n",
    "        nn.LeakyReLU(),\n",
    "        \n",
    "        # Layer 3: 512 â†’ 1024 (further expansion)\n",
    "        nn.Linear(512, 1024),\n",
    "        nn.LeakyReLU(),\n",
    "        \n",
    "        # Layer 4: 1024 â†’ 784 (reconstruct full image)\n",
    "        nn.Linear(1024, 28*28),\n",
    "        nn.Sigmoid()  # Squash pixels to [0, 1]\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass through encoder and decoder.\n",
    "    \n",
    "    Args:\n",
    "        x: Input images (batch, 1, 28, 28)\n",
    "    \n",
    "    Returns:\n",
    "        Reconstructed images (batch, 1, 28, 28)\n",
    "    \"\"\"\n",
    "    \n",
    "    original_shape = x.shape\n",
    "    \n",
    "    # Flatten image for linear layers: (batch, 1, 28, 28) â†’ (batch, 784)\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    \n",
    "    # Encode: (batch, 784) â†’ (batch, dim_z)\n",
    "    z = self.Encoder(x)\n",
    "    \n",
    "    # Decode: (batch, dim_z) â†’ (batch, 784)\n",
    "    x = self.Decoder(z)\n",
    "    \n",
    "    # Reshape back to image format: (batch, 784) â†’ (batch, 1, 28, 28)\n",
    "    x = x.view(original_shape)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ **Instantiating the Autoencoder Model**\n",
    "\n",
    "Now we create an instance of our autoencoder with a specific latent dimension.\n",
    "\n",
    "#### ðŸŽ›ï¸ **Latent Dimension (`dim_z`)**\n",
    "- **Value:** 128 dimensions\n",
    "- **Meaning:** Each 784-pixel image is compressed into just 128 numbers!\n",
    "- **Compression Ratio:** 784 / 128 = 6.125Ã— compression\n",
    "\n",
    "**Effect of Different `dim_z` Values:**\n",
    "- **Smaller (e.g., 32):** \n",
    "  - More aggressive compression\n",
    "  - Harder to train\n",
    "  - May lose fine details\n",
    "  - Better for learning core digit structure\n",
    "\n",
    "- **Larger (e.g., 256):**\n",
    "  - Less compression\n",
    "  - Easier to train\n",
    "  - Preserves more details\n",
    "  - Less efficient representation\n",
    "\n",
    "The 128-dimensional latent space strikes a good balance between compression and reconstruction quality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1698567000051,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "IFKJOAj5Pbdl"
   },
   "outputs": [],
   "source": [
    "# Set latent dimension (embedding size)\n",
    "dim_z = 128\n",
    "\n",
    "# Create autoencoder model\n",
    "model = AutoEncoderGenerator(dim_z=dim_z)\n",
    "\n",
    "print(f\"âœ… Model created with latent dimension: {dim_z}\")\n",
    "print(f\"ðŸ“Š Compression ratio: {28*28 / dim_z:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“‹ **Model Architecture Summary**\n",
    "\n",
    "Let's examine our model's architecture in detail using `torchsummary`.\n",
    "\n",
    "#### ðŸ“Š **What the Summary Shows**\n",
    "\n",
    "The summary provides:\n",
    "- **Layer-by-layer breakdown** of the architecture\n",
    "- **Output shapes** after each layer\n",
    "- **Parameter counts** (trainable weights and biases)\n",
    "- **Total parameters** and model size\n",
    "\n",
    "#### ðŸ” **Key Metrics to Observe**\n",
    "\n",
    "1. **Encoder Path:** Watch dimensions shrink from 784 â†’ 128\n",
    "2. **Decoder Path:** Watch dimensions expand from 128 â†’ 784\n",
    "3. **Parameter Count:** Total trainable parameters in millions\n",
    "4. **Memory Usage:** Estimated memory required for forward/backward pass\n",
    "\n",
    "This summary helps verify:\n",
    "- Architecture is correctly implemented\n",
    "- Dimensions flow properly through the network\n",
    "- Model size is reasonable for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1698567010373,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "YWBNQw3LtZd4",
    "outputId": "e6ce8962-1068-43aa-a521-6e62f67e4d24"
   },
   "outputs": [],
   "source": [
    "summary(model, (1, 28, 28), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ **Training Configuration and Setup**\n",
    "\n",
    "Before training, we configure our model, optimizer, loss function, and compute device.\n",
    "\n",
    "#### ðŸ–¥ï¸ **Device Selection (CPU vs GPU)**\n",
    "```python\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "```\n",
    "- **GPU (CUDA):** Dramatically faster training through parallel computation\n",
    "- **CPU:** Works everywhere but significantly slower\n",
    "- Model and all tensors must be on the same device\n",
    "\n",
    "#### ðŸŽ›ï¸ **Hyperparameters**\n",
    "\n",
    "**Number of Epochs:** 20\n",
    "- How many times to iterate through the entire dataset\n",
    "- More epochs â†’ better learning (but risk of overfitting)\n",
    "\n",
    "**Learning Rate:** 1e-4 (0.0001)\n",
    "- Controls step size during gradient descent\n",
    "- Small learning rate for stable training\n",
    "- Autoencoders typically need careful tuning\n",
    "\n",
    "#### ðŸ”§ **Optimizer: Adam**\n",
    "- Adaptive learning rate optimizer\n",
    "- Combines momentum and RMSprop benefits\n",
    "- Generally works well with minimal tuning\n",
    "- Well-suited for autoencoder training\n",
    "\n",
    "#### ðŸ“‰ **Loss Function: MSE (Mean Squared Error)**\n",
    "```python\n",
    "Loss = MSE(Original, Reconstructed) = mean((Original - Reconstructed)Â²)\n",
    "```\n",
    "\n",
    "**Why MSE for Autoencoders?**\n",
    "- Measures pixel-wise reconstruction error\n",
    "- Differentiable and smooth\n",
    "- Penalizes large errors more than small ones\n",
    "- Standard choice for image reconstruction tasks\n",
    "\n",
    "**Alternative Loss Functions:**\n",
    "- Binary Cross-Entropy: For binary images\n",
    "- L1 Loss (MAE): For sharper reconstructions\n",
    "- Perceptual Loss: For better visual quality (requires pre-trained network)\n",
    "\n",
    "Our setup is now ready for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2317,
     "status": "ok",
     "timestamp": 1698567344730,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "tXbOcut4fJFV"
   },
   "outputs": [],
   "source": [
    "# Configure device (GPU/CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "lr = 1e-4\n",
    "\n",
    "# Optimizer: Adam with learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Loss function: Mean Squared Error for reconstruction\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"ðŸ–¥ï¸  Device: {device}\")\n",
    "print(f\"ðŸ“Š Epochs: {num_epochs}\")\n",
    "print(f\"ðŸ“‰ Learning Rate: {lr}\")\n",
    "print(f\"ðŸŽ¯ Loss Function: MSE\")\n",
    "print(\"\\nâœ… Training configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ **Training Loop: Learning to Compress and Reconstruct**\n",
    "\n",
    "Now we train the autoencoder to minimize reconstruction error using batch gradient descent.\n",
    "\n",
    "#### ðŸ”„ **Training Process (Per Epoch)**\n",
    "\n",
    "For each epoch, we iterate through all training batches:\n",
    "\n",
    "**1. Forward Pass:**\n",
    "```python\n",
    "X â†’ Encoder â†’ Z (latent) â†’ Decoder â†’ X_hat (reconstructed)\n",
    "```\n",
    "\n",
    "**2. Loss Computation:**\n",
    "```python\n",
    "Loss = MSE(X, X_hat)  # How different is reconstruction from original?\n",
    "```\n",
    "\n",
    "**3. Backward Pass & Optimization:**\n",
    "```python\n",
    "optimizer.zero_grad()  # Clear old gradients\n",
    "loss.backward()        # Compute gradients via backpropagation\n",
    "optimizer.step()       # Update weights\n",
    "```\n",
    "\n",
    "**4. Tracking:**\n",
    "- Accumulate weighted loss across batches\n",
    "- Compute average epoch loss\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸŽ¯ **Training Objective**\n",
    "\n",
    "The model learns to:\n",
    "1. **Compress** images into meaningful latent representations\n",
    "2. **Reconstruct** images as accurately as possible from those representations\n",
    "\n",
    "**Key Insight:** The bottleneck forces the model to learn the most important features of digits â€” it can't just memorize!\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“Š **Expected Behavior**\n",
    "\n",
    "- **Loss should decrease** steadily over epochs\n",
    "- **Initial epochs:** Large loss (poor reconstruction)\n",
    "- **Later epochs:** Low loss (good reconstruction)\n",
    "- If loss plateaus: Model has converged\n",
    "- If loss fluctuates wildly: Learning rate may be too high\n",
    "\n",
    "---\n",
    "\n",
    "#### â±ï¸ **Training Duration**\n",
    "\n",
    "- **GPU:** ~2-5 minutes for 20 epochs\n",
    "- **CPU:** ~15-30 minutes for 20 epochs\n",
    "\n",
    "Let the training begin! ðŸŽ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319020,
     "status": "ok",
     "timestamp": 1698567711565,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "CFXvYta-rxsT",
    "outputId": "047387ff-4c70-49e1-d218-90877bf01c2b"
   },
   "outputs": [],
   "source": [
    "# List to track training loss over epochs\n",
    "train_losses = []\n",
    "\n",
    "print(\"ðŸš€ Starting training...\\n\")\n",
    "\n",
    "for i in range(num_epochs):\n",
    "  \n",
    "  epoch_weighted_loss = 0\n",
    "  \n",
    "  for (X, _) in train_loader:  # _ ignores labels (we don't need them!)\n",
    "    \n",
    "    # Move batch to device\n",
    "    X = X.to(device)\n",
    "    \n",
    "    # ============ FORWARD PASS ============\n",
    "    # X_hat because we're trying to reconstruct what's fed to it (X)\n",
    "    X_hat = model(X)  # X â†’ Encoder â†’ Decoder â†’ X_hat\n",
    "    \n",
    "    # ============ COMPUTE LOSS ============\n",
    "    loss = criterion(X_hat, X)  # MSE between reconstruction and original\n",
    "    \n",
    "    # ============ BACKWARD PASS ============\n",
    "    optimizer.zero_grad()  # Clear gradients from previous batch\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    # ============ TRACK LOSS ============\n",
    "    epoch_weighted_loss += loss.item() * len(X)\n",
    "  \n",
    "  # Calculate average loss for epoch\n",
    "  epoch_loss = epoch_weighted_loss / len(train_loader.dataset)\n",
    "  train_losses.append(epoch_loss)\n",
    "  \n",
    "  # Print progress\n",
    "  print(f'Epoch {i+1}/{num_epochs}, Loss = {epoch_loss:.6f}')\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ **Visualizing Training Progress: Loss Trajectory**\n",
    "\n",
    "After training, we plot the loss curve to understand the model's learning behavior.\n",
    "\n",
    "#### ðŸ” **Interpretation**\n",
    "\n",
    "**âœ… Ideal Behavior:**\n",
    "- **Steep decrease** in early epochs (rapid initial learning)\n",
    "- **Gradual decrease** in later epochs (fine-tuning)\n",
    "- **Smooth curve** (stable training)\n",
    "- **Plateau** near the end (convergence)\n",
    "\n",
    "**âš ï¸ Potential Issues:**\n",
    "\n",
    "1. **Loss decreases slowly:**\n",
    "   - Learning rate too small\n",
    "   - Model too simple (increase capacity)\n",
    "   - Need more epochs\n",
    "\n",
    "2. **Loss remains high:**\n",
    "   - Architecture limitations (linear layers only!)\n",
    "   - Bottleneck too small (increase `dim_z`)\n",
    "   - Complex patterns hard to capture\n",
    "\n",
    "3. **Loss fluctuates erratically:**\n",
    "   - Learning rate too large\n",
    "   - Batch size too small\n",
    "   - Training instability\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ’¡ **Expected Results**\n",
    "\n",
    "For our linear autoencoder:\n",
    "- Loss should reach ~0.02-0.05 range\n",
    "- Won't be perfect (no CNNs for spatial patterns!)\n",
    "- But good enough to see learned digit structures\n",
    "\n",
    "Let's see how our model performed! ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1698567720144,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "XUwJuIZosq1_",
    "outputId": "f3057800-3d66-4535-dabc-df3dd38e19b5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, linewidth=2, marker='o', color='#2E86AB')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Curve â€” Autoencoder Reconstruction Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ–¼ï¸ **Helper Function: Displaying Image Grids**\n",
    "\n",
    "Before generating images, we define a helper function to display multiple images in an organized grid.\n",
    "\n",
    "#### ðŸ“ **Function Purpose**\n",
    "\n",
    "`display_image_grid()` creates a clean, organized visualization of multiple images:\n",
    "- Arranges images in rows and columns\n",
    "- Removes axis labels for cleaner appearance\n",
    "- Adds an overall title\n",
    "- Uses grayscale colormap for MNIST digits\n",
    "\n",
    "#### ðŸŽ¨ **Parameters**\n",
    "- `images`: List of images to display\n",
    "- `num_rows`: Number of rows in the grid\n",
    "- `num_cols`: Number of columns in the grid\n",
    "- `title_text`: Title for the entire figure\n",
    "\n",
    "#### ðŸ’¡ **Why This Helper?**\n",
    "- Matplotlib's default subplot creation can be verbose\n",
    "- `ImageGrid` provides cleaner syntax and better spacing\n",
    "- Reusable for both reconstructions and generations\n",
    "\n",
    "This makes our result visualization much cleaner and more professional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1698567774272,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "0wBaVZ0UvkOG"
   },
   "outputs": [],
   "source": [
    "def display_image_grid(images, num_rows, num_cols, title_text):\n",
    "    \"\"\"\n",
    "    Display a grid of images with clean formatting.\n",
    "    \n",
    "    Args:\n",
    "        images: List/array of images to display\n",
    "        num_rows: Number of rows in the grid\n",
    "        num_cols: Number of columns in the grid\n",
    "        title_text: Title for the figure\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(num_cols*3., num_rows*3.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(num_rows, num_cols), axes_pad=0.15)\n",
    "    \n",
    "    for ax, im in zip(grid, images):\n",
    "        ax.imshow(im, cmap=\"gray\")\n",
    "        ax.axis(\"off\")  # Remove axis labels for cleaner look\n",
    "    \n",
    "    plt.suptitle(title_text, fontsize=20, fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¨ **Generating New Images from Random Latent Codes**\n",
    "\n",
    "Now comes the exciting part â€” using our trained decoder to generate completely new digit images!\n",
    "\n",
    "#### ðŸŽ² **Generation Process**\n",
    "\n",
    "**1. Sample Random Latent Codes:**\n",
    "```python\n",
    "sample_encodings = (torch.rand(N, dim_z) - 0.5) * 2  # Range: [-1, 1]\n",
    "```\n",
    "- Generate random values uniformly in [-1, 1]\n",
    "- Matches the range of our Tanh activation in the encoder\n",
    "- Each sample is a point in 128-dimensional latent space\n",
    "\n",
    "**2. Feed Through Decoder:**\n",
    "```python\n",
    "generations = model.Decoder(sample_encodings)\n",
    "```\n",
    "- **Bypass the encoder entirely!**\n",
    "- Decoder has learned to map latent space â†’ images\n",
    "- Each latent code produces a unique image\n",
    "\n",
    "**3. Reshape for Display:**\n",
    "```python\n",
    "generations = generations.reshape(-1, 28, 28, 1)\n",
    "```\n",
    "- Convert from (N, 784) to (N, 28, 28, 1)\n",
    "- Now in image format for visualization\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ” **What to Expect**\n",
    "\n",
    "**âœ… Good Signs:**\n",
    "- Some images resemble recognizable digits\n",
    "- Smooth transitions between different digit shapes\n",
    "- Consistent style across generations\n",
    "\n",
    "**âš ï¸ Expected Limitations:**\n",
    "- Images may be blurry (linear layers only!)\n",
    "- Some generations may be unclear or ambiguous\n",
    "- Not all samples will be perfect digits\n",
    "- Some may be \"in-between\" digits (e.g., 3-8 hybrid)\n",
    "\n",
    "**Why?** Our model:\n",
    "- Uses only linear layers (no spatial awareness)\n",
    "- Has a compressed latent space\n",
    "- Samples from random points (not necessarily \"valid\" digit regions)\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ’¡ **Key Insight**\n",
    "\n",
    "Even with these limitations, if we can identify **any** recognizable digits, it proves our autoencoder has learned meaningful patterns in the latent space!\n",
    "\n",
    "Let's see what our model generates! ðŸŽ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "executionInfo": {
     "elapsed": 1152,
     "status": "ok",
     "timestamp": 1698567787840,
     "user": {
      "displayName": "Telha Bin Bilal",
      "userId": "04147761314111353462"
     },
     "user_tz": -300
    },
    "id": "aO3WkbTiuQQM",
    "outputId": "7a9bbeb0-e375-4636-d6bf-591c64bf18bc"
   },
   "outputs": [],
   "source": [
    "# Configuration for generation\n",
    "rows, cols = 2, 7  # Generate 14 images (2 rows Ã— 7 columns)\n",
    "\n",
    "# Sample random points in latent space [-1, 1]\n",
    "sample_encodings = (torch.rand(rows*cols, dim_z).to(device) - 0.5) * 2\n",
    "\n",
    "print(f\"ðŸŽ² Sampling {rows*cols} random latent codes...\")\n",
    "print(f\"ðŸ“Š Latent code shape: {sample_encodings.shape}\")\n",
    "\n",
    "# Generate images from random latent codes\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    generations = model.Decoder(sample_encodings).cpu()\n",
    "    generations = generations.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(\"âœ… Generation complete!\\n\")\n",
    "\n",
    "# Display generated images in a grid\n",
    "display_image_grid(generations, rows, cols, \"Generated Images from Random Latent Codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi16wGLUkWJa"
   },
   "source": [
    "### ðŸ” **Interpreting Generated Results**\n",
    "\n",
    "Let's analyze what our autoencoder has learned and what the generated images tell us.\n",
    "\n",
    "#### âœ… **What We Can Observe**\n",
    "\n",
    "**Positive Indicators:**\n",
    "- Some images show **recognizable digit shapes**\n",
    "- The model has learned **basic digit structures** (curves, lines, loops)\n",
    "- Generated images have **consistent style** (grayscale, centered)\n",
    "- Some samples clearly resemble specific digits (e.g., 0, 1, 6, 8)\n",
    "\n",
    "**Expected Limitations:**\n",
    "- Images may be **blurry or unclear**\n",
    "- Some generations are **ambiguous** (hard to tell which digit)\n",
    "- Not all samples look like valid digits\n",
    "- Some may appear as \"blends\" of multiple digits\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§  **Why These Limitations?**\n",
    "\n",
    "**1. Linear Layers Only:**\n",
    "- No convolutional layers for spatial pattern recognition\n",
    "- No explicit modeling of local pixel relationships\n",
    "- Each pixel treated independently (not optimal for images)\n",
    "\n",
    "**2. Limited Training:**\n",
    "- 20 epochs is relatively short\n",
    "- More training might improve quality\n",
    "- But fundamental architectural limitations remain\n",
    "\n",
    "**3. Random Sampling:**\n",
    "- We're sampling **uniformly** from [-1, 1]\n",
    "- True digit representations may cluster in specific regions\n",
    "- Some random points may be \"between\" valid digits\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸŽ¯ **Success Criteria**\n",
    "\n",
    "Even with limitations, our experiment is successful if:\n",
    "- âœ… **Any** images are recognizable as digits\n",
    "- âœ… Model learned **compressed representations**\n",
    "- âœ… Decoder can **generate** from latent codes\n",
    "- âœ… Generated images have **digit-like characteristics**\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ’¡ **Key Takeaway**\n",
    "\n",
    "> *\"The images aren't perfect, but we can see it's trying to learn. In several images, we can guess which digit it's attempting to generate!\"*\n",
    "\n",
    "This demonstrates that even a simple linear autoencoder can capture meaningful patterns in handwritten digits. The model has successfully learned:\n",
    "- **Compression:** 784 pixels â†’ 128 dimensions\n",
    "- **Reconstruction:** 128 dimensions â†’ 784 pixels\n",
    "- **Generation:** Random codes â†’ digit-like images\n",
    "\n",
    "With CNN-based architectures, results would be dramatically sharper and more realistic!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1ihkxgohR_9zOcQafaT4lj6p-45_iu6rB",
     "timestamp": 1698562874692
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
