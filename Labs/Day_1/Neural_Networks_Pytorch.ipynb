{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p0F7eA7MVkI"
   },
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zosysXEi3Xnl"
   },
   "source": [
    "## ðŸ’¡ **Introduction**\n",
    "\n",
    "In this lab, we explore how to build, train, and interpret a **neural network for regression** using **PyTorch**.  \n",
    "Our goal is to predict the **aqueous solubility (`logS`)** of chemical compounds based on their molecular descriptors.  \n",
    "Through this hands-on exercise, we demonstrate the complete workflow of a machine learning project â€” from data preprocessing to model evaluation and interpretation.\n",
    "\n",
    "The dataset, obtained from the **Delaney solubility dataset**, contains key molecular properties such as:\n",
    "- `MolLogP`: Hydrophobicity measure (log partition coefficient)  \n",
    "- `MolWt`: Molecular weight  \n",
    "- `NumRotatableBonds`: Structural flexibility indicator  \n",
    "- `AromaticProportion`: Ratio of aromatic atoms  \n",
    "\n",
    "By combining these features, we train a neural network to learn the non-linear relationship between molecular structure and solubility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UASypbRN32x3"
   },
   "source": [
    "### ðŸ§  **Importing Libraries and Frameworks**\n",
    "\n",
    "In this step, we import all the essential Python libraries that will be used throughout the lab:\n",
    "\n",
    "- **NumPy** (`np`) â€” for numerical operations such as array manipulation.  \n",
    "- **Pandas** (`pd`) â€” for data handling and preprocessing using DataFrames.  \n",
    "- **Matplotlib** (`plt`) â€” for visualizing data and results.  \n",
    "- **PyTorch** (`torch`, `torch.nn`, `torch.optim`) â€” for building, training, and optimizing neural networks.  \n",
    "- **Dataset** and **DataLoader** â€” from PyTorchâ€™s `torch.utils.data` to manage our data efficiently during training.  \n",
    "- **Scikit-learn** (`sklearn`) â€” specifically:\n",
    "  - `train_test_split` for splitting data into training and testing sets.\n",
    "  - `StandardScaler` for normalizing features to improve model convergence.\n",
    "\n",
    "These imports set up the foundational tools weâ€™ll need for creating and training our neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1761486408319,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "vGl_rHxSMlHF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boTgTO7U3-bK"
   },
   "source": [
    "### ðŸ“Š **Loading and Preparing the Dataset**\n",
    "\n",
    "In this section, we load and preprocess the dataset used for training our neural network.\n",
    "\n",
    "1. **Dataset Source:**  \n",
    "   The data is loaded directly from a public GitHub repository. It contains molecular descriptors and solubility values (`logS`) for various compounds.  \n",
    "\n",
    "2. **Feature and Target Selection:**  \n",
    "   - **Features (`X`)**:  \n",
    "     - `MolLogP`: Logarithm of the partition coefficient (hydrophobicity).  \n",
    "     - `MolWt`: Molecular weight.  \n",
    "     - `NumRotatableBonds`: Number of rotatable bonds in the molecule.  \n",
    "     - `AromaticProportion`: Ratio of aromatic atoms to total heavy atoms.  \n",
    "   - **Target (`y`)**:  \n",
    "     - `logS`: The solubility of the molecule (log scale).\n",
    "\n",
    "3. **Data Splitting:**  \n",
    "   We split the dataset into **training (80%)** and **validation (20%)** sets using `train_test_split` to evaluate our model's generalization.\n",
    "\n",
    "4. **Normalization:**  \n",
    "   Since features vary in scale, we use `StandardScaler` to normalize them â€” ensuring each feature has **zero mean** and **unit variance**, which helps the neural network converge more efficiently.\n",
    "\n",
    "Finally, the shapes of the training and validation sets are printed to confirm successful preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1761486408481,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "Yl1Y_UWmkP-A",
    "outputId": "6e2b239a-625e-4d6e-87bd-d9ba932a4397"
   },
   "outputs": [],
   "source": [
    "# --- Load dataset ---\n",
    "data = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv'\n",
    ")\n",
    "\n",
    "feature_cols = [\"MolLogP\", \"MolWt\", \"NumRotatableBonds\", \"AromaticProportion\"]\n",
    "target_col   = \"logS\"\n",
    "\n",
    "X = data[feature_cols].values\n",
    "y = data[[target_col]].values\n",
    "\n",
    "# --- Split (train/val) ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Normalize using sklearn StandardScaler ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "print(\"Train shape:\", X_train_scaled.shape, \"Val shape:\", X_val_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkmtIiUHe-ib"
   },
   "source": [
    "### ðŸ§© **Creating a Custom PyTorch Dataset**\n",
    "\n",
    "Here we define a custom dataset class named `SolubilityDataset`, which extends PyTorchâ€™s built-in `Dataset` class.  \n",
    "This allows us to efficiently handle tabular data (`X`, `y`) and later feed it into a `DataLoader` for batching and shuffling.\n",
    "\n",
    "#### ðŸ” **Class Breakdown**\n",
    "- **`__init__(self, X, y)`**  \n",
    "  - Converts input NumPy arrays (`X`, `y`) into PyTorch tensors of type `float32`.  \n",
    "  - This ensures compatibility with PyTorch models and GPU computation.\n",
    "\n",
    "- **`__len__(self)`**  \n",
    "  - Returns the total number of samples in the dataset.  \n",
    "  - Used internally by PyTorch to know how many batches can be created.\n",
    "\n",
    "- **`__getitem__(self, idx)`**  \n",
    "  - Retrieves one sample (features and target) at a given index `idx`.  \n",
    "  - Enables easy iteration over data during training and validation.\n",
    "\n",
    "This structure standardizes how data is accessed â€” making it clean, reusable, and compatible with PyTorchâ€™s training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1761486408481,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "W07NRFgRVjcJ"
   },
   "outputs": [],
   "source": [
    "class SolubilityDataset(Dataset):\n",
    "    \"\"\"Basic dataset for tabular (X, y) regression.\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmGMfZkUfdTB"
   },
   "source": [
    "### ðŸ§º **Creating DataLoaders for Training and Validation**\n",
    "\n",
    "Now that weâ€™ve defined our custom dataset, we use PyTorchâ€™s `DataLoader` to handle batching and shuffling of our data.\n",
    "\n",
    "#### âš™ï¸ **Steps Explained**\n",
    "\n",
    "1. **Batch Size:**  \n",
    "   We define `batch_size = 32`, meaning the model will process 32 samples at a time before updating its weights.\n",
    "\n",
    "2. **Dataset Creation:**  \n",
    "   - `train_dataset` and `val_dataset` are created using our `SolubilityDataset` class.  \n",
    "   - They wrap the preprocessed training and validation data respectively.\n",
    "\n",
    "3. **DataLoader Setup:**  \n",
    "   - `train_loader`: Loads training data in batches of 32 and **shuffles** it each epoch to improve learning.  \n",
    "   - `val_loader`: Loads validation data in batches of 32 **without shuffling**, since evaluation doesnâ€™t require randomization.\n",
    "\n",
    "4. **Sanity Check:**  \n",
    "  prints the shape of one batch of features (`X`) and targets (`y`) to confirm the DataLoader works correctly.\n",
    "\n",
    "This setup enables efficient data feeding to the neural network during both training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1761486408482,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "JiNJXjYyVmmB",
    "outputId": "f714377b-8a1a-40d7-839f-30524e8f24e3"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = SolubilityDataset(X_train_scaled, y_train)\n",
    "val_dataset   = SolubilityDataset(X_val_scaled,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# quick sanity check\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch X:\", xb.shape, \"Batch y:\", yb.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T50vc_q9f-av"
   },
   "source": [
    "### ðŸ§± **Defining the Neural Network (Layer-by-Layer)**\n",
    "\n",
    "In this section, we build the `SolubilityNet` model **manually layer by layer.\n",
    "\n",
    "#### ðŸ§© **Architecture Overview**\n",
    "\n",
    "| Layer | Type | Input â†’ Output | Purpose |\n",
    "|:------|:------|:----------------|:--------|\n",
    "| 1 | `Linear(4, 32)` | 4 â†’ 32 | First dense layer taking molecular features as input |\n",
    "| 2 | `LeakyReLU()` | â€” | Non-linear activation preventing dead neurons |\n",
    "| 3 | `Dropout(0.2)` | â€” | Regularization layer to reduce overfitting |\n",
    "| 4 | `Linear(32, 16)` | 32 â†’ 16 | Second dense layer |\n",
    "| 5 | `LeakyReLU()` | â€” | Adds non-linearity again |\n",
    "| 6 | `Dropout(0.2)` | â€” | Further regularization |\n",
    "| 7 | `Linear(16, 8)` | 16 â†’ 8 | Third dense layer |\n",
    "| 8 | `LeakyReLU()` | â€” | Activation to introduce non-linear patterns |\n",
    "| 9 | `Linear(8, 1)` | 8 â†’ 1 | Output layer producing the solubility prediction (`logS`) |\n",
    "\n",
    "#### âš™ï¸ **Forward Pass**\n",
    "The `forward()` method defines how input data flows through each layer â€”  \n",
    "from raw molecular descriptors to the final predicted solubility value.\n",
    "\n",
    "This explicit implementation helps you understand **how data transforms step by step** inside the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1761486408482,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "9QEKqXkLVpFM",
    "outputId": "6cb14f5c-6e81-4778-f88f-9c59e278a3ce"
   },
   "outputs": [],
   "source": [
    "class SolubilityNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers explicitly\n",
    "        self.fc1 = nn.Linear(4, 16)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.act1(self.fc1(x)))  # Input â†’ Hidden layer 1\n",
    "        x = self.act2(self.fc2(x))               # Hidden layer 2\n",
    "        x = self.fc3(x)                          # Output layer\n",
    "        return x\n",
    "\n",
    "model = SolubilityNet()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nTgC1n8hpi4"
   },
   "source": [
    "### âš™ï¸ **Loss Function and Optimizer**\n",
    "\n",
    "We set up how the model learns and updates its weights during training.\n",
    "\n",
    "- **Loss Function:**  \n",
    "  `nn.MSELoss()` measures the average squared difference between predicted and actual solubility (`logS`) values â€” suitable for regression.\n",
    "\n",
    "- **Optimizer:**  \n",
    "  `optim.AdamW(model.parameters(), lr=0.01)` updates weights using the AdamW algorithm, which combines adaptive learning rates with weight decay to improve stability and prevent overfitting.\n",
    "\n",
    "These choices help the model minimize prediction error efficiently during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1761486408506,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "Evu-gpTbV0gw"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGI_qJLdhuf8"
   },
   "source": [
    "### ðŸ” **Training the Neural Network**\n",
    "\n",
    "In this section, we train our model to learn the relationship between molecular descriptors and solubility (`logS`).\n",
    "\n",
    "#### ðŸ§  **Training Setup**\n",
    "- **Epochs:** We train for `25` complete passes over the training data.  \n",
    "- **Trackers:** Two lists (`train_losses`, `val_losses`) store average losses for each epoch to monitor progress.\n",
    "\n",
    "#### âš™ï¸ **Training Loop**\n",
    "For each epoch:\n",
    "1. **Training Phase (`model.train()`):**\n",
    "   - Loops through all batches from `train_loader`.\n",
    "   - Performs forward pass â†’ computes loss â†’ backpropagates gradients â†’ updates weights.\n",
    "   - Collects average training loss for the epoch.\n",
    "\n",
    "2. **Validation Phase (`model.eval()`):**\n",
    "   - Disables gradient tracking using `torch.no_grad()`.\n",
    "   - Evaluates the model on unseen validation data.\n",
    "   - Computes and stores the average validation loss.\n",
    "\n",
    "Finally, each epoch prints both training and validation losses, allowing us to track how well the model is learning and whether itâ€™s overfitting or improving steadily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1249,
     "status": "ok",
     "timestamp": 1761486409768,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "QkUxLtlcV2Hb",
    "outputId": "0ee8a942-27c5-49e2-e36b-41a80208d6ce"
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---- Training phase ----\n",
    "    model.train()\n",
    "    batch_train_losses = []\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_train_losses.append(loss.item())\n",
    "\n",
    "    train_losses.append(np.mean(batch_train_losses))\n",
    "\n",
    "    # ---- Validation phase ----\n",
    "    model.eval()\n",
    "    batch_val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb)\n",
    "            vloss = criterion(preds, yb)\n",
    "            batch_val_losses.append(vloss.item())\n",
    "    val_losses.append(np.mean(batch_val_losses))\n",
    "    print(f\"Epoch {epoch:03d} | Train {train_losses[-1]:.4f} | Val {val_losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdqUJ2FGh2bM"
   },
   "source": [
    "### ðŸ“ˆ **Visualizing Training and Validation Loss**\n",
    "\n",
    "After training, we plot the loss curves to observe how the modelâ€™s performance evolved over time.\n",
    "\n",
    "- **Training Loss (Train MSE):** Indicates how well the model fits the training data.  \n",
    "- **Validation Loss (Val MSE):** Shows how well the model generalizes to unseen data.\n",
    "\n",
    "#### ðŸ” **Interpretation**\n",
    "- A smooth **decrease in both losses** suggests effective learning.  \n",
    "- If validation loss starts increasing while training loss decreases, it signals **overfitting**.\n",
    "\n",
    "This plot helps confirm whether the model converged properly and provides insights into its learning dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1761486409985,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "IMnELB0pWBk7",
    "outputId": "261f6538-9c89-405a-819a-6f1eb0d741e4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Train MSE\")\n",
    "plt.plot(val_losses, label=\"Val MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training Curve â€” Solubility Prediction\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF1UMfeJh4xm"
   },
   "source": [
    "### ðŸ§© **Feature Importance via Permutation Test**\n",
    "\n",
    "To understand which molecular descriptors most influence solubility predictions, we use a **permutation importance** approach.\n",
    "\n",
    "#### âš™ï¸ **How It Works**\n",
    "1. Compute the **base loss** on the validation set (normal, unshuffled data).  \n",
    "2. For each feature in `feature_cols`:\n",
    "   - Shuffle that featureâ€™s values while keeping others fixed.  \n",
    "   - Recalculate the modelâ€™s loss (`perm_loss`).  \n",
    "   - The **increase in loss (Î”Loss)** indicates how important that feature is â€”  \n",
    "     a larger Î”Loss means the feature had a stronger impact on predictions.\n",
    "\n",
    "#### ðŸ§  **Interpretation**\n",
    "By comparing how much the loss rises when each feature is shuffled, we can rank features by their predictive power.  \n",
    "This provides a simple yet effective way to interpret the neural networkâ€™s behavior on tabular data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1761486410011,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "NQfN9sXTWDak",
    "outputId": "3b6c2136-e3b4-4140-e0b2-64f650b8b446"
   },
   "outputs": [],
   "source": [
    "base_loss = criterion(model(torch.tensor(X_val_scaled, dtype=torch.float32)),\n",
    "                      torch.tensor(y_val, dtype=torch.float32)).item()\n",
    "\n",
    "for i, name in enumerate(feature_cols):\n",
    "    X_perm = X_val_scaled.copy()\n",
    "    np.random.shuffle(X_perm[:, i])\n",
    "    with torch.no_grad():\n",
    "        perm_loss = criterion(model(torch.tensor(X_perm, dtype=torch.float32)),\n",
    "                              torch.tensor(y_val, dtype=torch.float32)).item()\n",
    "    print(f\"{name:20s} Î”Loss: {perm_loss - base_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxMEwVITil1c"
   },
   "source": [
    "### ðŸ§¾ **Evaluating Model Predictions**\n",
    "\n",
    "In this step, we evaluate how well the trained model predicts solubility (`logS`) on the **validation set**.\n",
    "\n",
    "#### âš™ï¸ **Process Overview**\n",
    "1. Switch the model to **evaluation mode** using `model.eval()` â€” this disables dropout and other training-only behaviors.  \n",
    "2. Use `torch.no_grad()` to perform inference efficiently without tracking gradients.  \n",
    "3. Generate predictions (`y_pred_val`) on the validation features.  \n",
    "4. Print a small comparison table showing the **true vs predicted** solubility values for the first few samples.\n",
    "\n",
    "This provides a quick and intuitive check of the modelâ€™s performance on unseen data before moving to more detailed metrics or visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1761486410039,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "9iHu86yDiMem",
    "outputId": "3c49bd37-febc-467d-fdf6-92489f3b7f19"
   },
   "outputs": [],
   "source": [
    "# --- Evaluate model on validation set ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "    y_pred_val = model(X_val_tensor)\n",
    "\n",
    "# --- Print results ---\n",
    "print(f\"{'True logS':>12} | {'Predicted logS':>14}\")\n",
    "print(\"-\" * 29)\n",
    "\n",
    "for i in range(min(5, len(y_val_tensor))):\n",
    "    print(f\"{y_val_tensor[i, 0]:12.3f} | {y_pred_val[i, 0]:14.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfYxy0RSi2-L"
   },
   "source": [
    "\n",
    "## ðŸ§¾ **Summary**\n",
    "\n",
    "In this experiment, we:\n",
    "1. **Loaded and preprocessed** the dataset using `pandas` and `scikit-learn`, applying normalization for better training stability.  \n",
    "2. **Created a custom PyTorch dataset and DataLoader** to efficiently handle batching and shuffling of data.  \n",
    "3. **Built a feedforward neural network** (`SolubilityNet`) with multiple dense layers, LeakyReLU activations, and dropout regularization.  \n",
    "4. **Trained the model** using the **Mean Squared Error (MSE)** loss function and the **AdamW optimizer** over multiple epochs, tracking both training and validation losses.  \n",
    "5. **Visualized training progress** using a loss curve to confirm proper convergence and detect potential overfitting.  \n",
    "6. **Analyzed feature importance** using permutation testing to understand which molecular descriptors most affected solubility predictions.\n",
    "\n",
    "Overall, this lab provided a clear, end-to-end example of **regression with neural networks in PyTorch**, emphasizing both **model implementation** and **interpretability**, a crucial aspect in real-world scientific applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1761486410040,
     "user": {
      "displayName": "Yazan Alshuaibi",
      "userId": "17695359026075763294"
     },
     "user_tz": -180
    },
    "id": "CJFrGX_ri9Ip"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPO0aCJUwN3Ct9AU0lrzO+f",
   "provenance": [
    {
     "file_id": "1ZgtZqNXFyS2YvSXG9gOgvakkCkQoXYk0",
     "timestamp": 1761478432026
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
