{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’¡ **Introduction**\n",
        "\n",
        "In this lab, we explore how to build, train, and evaluate a **neural network for image classification** using **PyTorch**.  \n",
        "Our goal is to classify handwritten digits (0-9) from the **MNIST dataset** using PyTorch's powerful deep learning framework.  \n",
        "Through this hands-on exercise, we demonstrate the complete workflow of building neural networks with modern tools â€” from model architecture to training loops and GPU acceleration.\n",
        "\n",
        "The dataset consists of **28Ã—28 grayscale images** of handwritten digits, where each pixel value ranges from 0 to 255.  \n",
        "\n",
        "#### ðŸŽ¯ **What We'll Build**\n",
        "- **1-Layer Neural Network:** Direct mapping from input to output with softmax activation\n",
        "- **2-Layer Neural Network:** Input â†’ Hidden Layer (ReLU) â†’ Output (Softmax)\n",
        "\n",
        "#### ðŸ”‘ **Key PyTorch Concepts**\n",
        "- **Automatic Differentiation:** PyTorch handles backpropagation automatically\n",
        "- **GPU Acceleration:** Train models faster using CUDA-enabled GPUs\n",
        "- **Modular Design:** Build reusable model classes with `nn.Module`\n",
        "- **Built-in Optimizers:** Use Adam optimizer for efficient weight updates\n",
        "- **Cross-Entropy Loss:** Standard loss function for multi-class classification\n",
        "\n",
        "By leveraging PyTorch's high-level abstractions, we can focus on model design while the framework handles the computational complexities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§  **Importing Libraries and Frameworks**\n",
        "\n",
        "In this step, we import all the essential Python libraries for our PyTorch-based neural network implementation:\n",
        "\n",
        "- **PyTorch** (`torch`) â€” the core deep learning framework for tensor operations and automatic differentiation.  \n",
        "- **torch.nn** â€” contains neural network layers, activation functions, and loss functions.  \n",
        "- **torch.optim** (`Adam`) â€” provides optimization algorithms for updating model weights.  \n",
        "- **torchvision.datasets** (`MNIST`) â€” for easily downloading and loading the MNIST dataset.  \n",
        "- **torch.utils.data** (`DataLoader`) â€” for creating batches of data efficiently during training.  \n",
        "- **torchvision.transforms.functional** (`to_tensor`) â€” to convert PIL images to tensor format.  \n",
        "- **Matplotlib** (`plt`) â€” for visualizing training curves and results.  \n",
        "- **IPython.display** (`clear_output`) â€” to clean up console output for better readability.\n",
        "\n",
        "These imports provide everything we need to build, train, and evaluate our neural network using PyTorch's powerful ecosystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFs5q8XAox82"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKSL__IwozZs"
      },
      "outputs": [],
      "source": [
        "# Download the required libraries (needed when running outside colab where the environment doesn't come pre-loaded with libraries)\n",
        "\n",
        "%pip install torch\n",
        "%pip install matplotlib\n",
        "%pip install torchvision\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xziZ2Kj1pZrq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tH9YmdSAW7i"
      },
      "source": [
        "### ðŸ“‹ **Lab Contents and Overview**\n",
        "\n",
        "In this notebook, we'll implement **1-layer and 2-layer neural networks** in PyTorch for classifying the MNIST dataset.\n",
        "\n",
        "#### ðŸ“· **About MNIST**\n",
        "The dataset consists of **28Ã—28 grayscale images**, each containing a handwritten digit from 0 to 9.  \n",
        "Our model needs to take these images and classify them to the correct digit.\n",
        "\n",
        "#### ðŸ› ï¸ **Prerequisites**\n",
        "**Required Knowledge:**\n",
        "1. **PyTorch** fundamentals (tensors, autograd, nn.Module)\n",
        "2. **Matplotlib** basics (for visualization)\n",
        "\n",
        "**Good to Have:**\n",
        "1. Understanding of `torch.utils.data.Dataset` and `DataLoader`\n",
        "2. Familiarity with GPU computing concepts (CUDA)\n",
        "\n",
        "#### ðŸŽ“ **Learning Objectives**\n",
        "By the end of this lab, you will:\n",
        "- Build custom neural network architectures using `nn.Module`\n",
        "- Train models with automatic differentiation\n",
        "- Use GPU acceleration for faster training\n",
        "- Implement proper train/validation loops\n",
        "- Save and load trained models\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“Š **Loading and Preparing the MNIST Dataset**\n",
        "\n",
        "In this section, we load and prepare the MNIST dataset for training and testing our neural network.\n",
        "\n",
        "#### 1ï¸âƒ£ **Dataset Loading**\n",
        "- The `MNIST` function from `torchvision.datasets` automatically downloads the dataset if not already present.\n",
        "- **Training set:** 60,000 images\n",
        "- **Test set:** 10,000 images\n",
        "\n",
        "#### 2ï¸âƒ£ **Data Transformation**\n",
        "- `transform=to_tensor` converts PIL images to PyTorch tensors\n",
        "- This transformation:\n",
        "  - Converts pixel values from [0, 255] to [0.0, 1.0]\n",
        "  - Changes format from (H, W, C) to (C, H, W) â€” PyTorch's expected format\n",
        "  - Enables GPU acceleration and automatic differentiation\n",
        "\n",
        "#### 3ï¸âƒ£ **Important Note**\n",
        "We're using PyTorch's data utilities for dataset handling, which provides:\n",
        "- Automatic downloading and caching\n",
        "- Easy batching with `DataLoader`\n",
        "- Efficient memory management\n",
        "- Support for parallel data loading\n",
        "\n",
        "The images will later be flattened from (1, 28, 28) to (784,) when fed into our fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTqeY0V6pV3T"
      },
      "outputs": [],
      "source": [
        "# MNIST function fetches the MNIST dataset. Without any transform param, the returned object is a Pillow image but we want to convert it to numerical form\n",
        "# that is to say, a numpy array/torch tensor\n",
        "\n",
        "# to_tensor is used to avoid errors when creating data loader later. we'll convert them to numpy arrays when the time comes\n",
        "train_data = MNIST(root='./datasets', train=True, download=True, transform=to_tensor)\n",
        "test_data  = MNIST(root='./datasets', train=False, download=True, transform=to_tensor)\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ”„ **Creating DataLoaders for Batch Processing**\n",
        "\n",
        "DataLoaders are essential PyTorch utilities that handle batching, shuffling, and efficient data loading during training.\n",
        "\n",
        "#### ðŸ“¦ **Why Use DataLoaders?**\n",
        "- **Batch Gradient Descent:** Process multiple samples simultaneously for faster convergence\n",
        "- **Memory Efficiency:** Load data in chunks rather than all at once\n",
        "- **Automatic Shuffling:** Randomize data order each epoch (when `shuffle=True`)\n",
        "- **Parallel Loading:** Use multiple workers for faster data preprocessing (via `num_workers` parameter)\n",
        "\n",
        "#### âš™ï¸ **Configuration**\n",
        "- **Batch Size:** 64 samples per batch\n",
        "  - Larger batches â†’ faster training but more memory usage\n",
        "  - Smaller batches â†’ more stable gradients but slower training\n",
        "\n",
        "The DataLoader will automatically:\n",
        "1. Group samples into batches of size 64\n",
        "2. Return tensors of shape `(64, 1, 28, 28)` for images\n",
        "3. Return tensors of shape `(64,)` for labels\n",
        "\n",
        "This makes our training loop clean and efficient!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWmnHXbSr9Al"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Dataloaders are used to easily create batches of data so we can perform batch gradient descent for faster learning\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aQrW8JC0HBo"
      },
      "source": [
        "### ðŸ—ï¸ **Model Architectures: 1-Layer vs 2-Layer Networks**\n",
        "\n",
        "Let's create the architecture of our models using PyTorch's `nn.Module` class.\n",
        "\n",
        "#### ðŸ” **Softmax vs Sigmoid**\n",
        "Instead of sigmoid activation (used in NumPy implementation), we'll use **Softmax** activation:\n",
        "\n",
        "- **Sigmoid:** Maps each value independently to range [0, 1]\n",
        "  - Output values don't sum to 1\n",
        "  - Each output is independent\n",
        "  \n",
        "- **Softmax:** Converts logits into a probability distribution\n",
        "  - Output values sum to 1\n",
        "  - Highest input value remains highest in output\n",
        "  - Perfect for multi-class classification\n",
        "\n",
        "**Softmax Formula:**\n",
        "$$\n",
        "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
        "$$\n",
        "\n",
        "#### ðŸ“ **Model 1: Single-Layer Network**\n",
        "Input (784) â†’ Linear â†’ Softmax â†’ Output (10)\n",
        "\n",
        "\n",
        "- **Simplest possible classifier**\n",
        "- Learns direct mapping from pixels to classes\n",
        "- No hidden representations\n",
        "- Limited expressiveness but fast to train\n",
        "\n",
        "#### ðŸ“ **Model 2: Two-Layer Network**\n",
        "\n",
        "- **Hidden layer** learns intermediate features\n",
        "- **ReLU activation** introduces non-linearity\n",
        "- More expressive than single layer\n",
        "- Comparable to our NumPy implementation\n",
        "\n",
        "\n",
        "#### ðŸ§© **PyTorch Module Structure**\n",
        "Each model inherits from `nn.Module` and must implement:\n",
        "1. **`__init__()`** â€” Define layers and operations\n",
        "2. **`forward()`** â€” Specify forward pass computation\n",
        "\n",
        "PyTorch automatically handles:\n",
        "- Backpropagation (via autograd)\n",
        "- Parameter management\n",
        "- GPU transfer\n",
        "- Gradient computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXfUV5ZLzGKa"
      },
      "outputs": [],
      "source": [
        "class NN1Layer(nn.Module):\n",
        "  \"\"\"\n",
        "  Single-layer neural network for MNIST classification.\n",
        "  Architecture: Input â†’ Linear â†’ Softmax â†’ Output\n",
        "  \"\"\"\n",
        "  def __init__(self, num_inp, num_out):\n",
        "    \n",
        "    super(NN1Layer, self).__init__()\n",
        "    \n",
        "    # Single fully connected layer\n",
        "    self.layer_1 = nn.Linear(num_inp, num_out)\n",
        "    self.softmax = nn.Softmax(dim=1)  # Apply softmax across class dimension\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \n",
        "    z = self.layer_1(x)\n",
        "    a = self.softmax(z)\n",
        "    \n",
        "    return a\n",
        "\n",
        "\n",
        "class NN2Layer(nn.Module):\n",
        "  \"\"\"\n",
        "  Two-layer neural network for MNIST classification.\n",
        "  Architecture: Input â†’ Linear â†’ ReLU â†’ Linear â†’ Softmax â†’ Output\n",
        "  \"\"\"\n",
        "  def __init__(self, num_inp, num_hidden, num_out):\n",
        "    \n",
        "    super(NN2Layer, self).__init__()\n",
        "    \n",
        "    # Define layers\n",
        "    self.layer_1 = nn.Linear(num_inp, num_hidden)\n",
        "    self.layer_2 = nn.Linear(num_hidden, num_out)\n",
        "    \n",
        "    # Activation functions\n",
        "    self.hidden_activation = nn.ReLU()  # We can change the hidden activation here (try Sigmoid, Tanh, etc.)\n",
        "    self.softmax = nn.Softmax(dim=1)    # dim 0 is batch size, we apply softmax across classes (dim 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \n",
        "    # Hidden layer\n",
        "    z1 = self.layer_1(x)\n",
        "    a1 = self.hidden_activation(z1)\n",
        "    \n",
        "    # Output layer\n",
        "    z2 = self.layer_2(a1)\n",
        "    a2 = self.softmax(z2)\n",
        "    \n",
        "    return a2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u_e7ldrtHh1"
      },
      "source": [
        "### âš™ï¸ **Training Setup: Model, Optimizer, and Device Configuration**\n",
        "\n",
        "Before training, we need to configure our model, optimizer, loss function, and compute device.\n",
        "\n",
        "#### ðŸŽ›ï¸ **Hyperparameters**\n",
        "- **Number of Epochs:** 12 â€” how many times to iterate through the entire dataset\n",
        "- **Learning Rate:** 1e-4 (0.0001) â€” controls step size during optimization\n",
        "  - Small learning rate â†’ stable but slow training\n",
        "  - Large learning rate â†’ fast but may overshoot optimal values\n",
        "\n",
        "#### ðŸ–¥ï¸ **Device Selection (CPU vs GPU)**\n",
        "```python\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "```\n",
        "- **CUDA (GPU):** Dramatically faster training via parallel computation\n",
        "- **CPU:** Works everywhere but significantly slower\n",
        "- All tensors and the model must be moved to the same device\n",
        "\n",
        "**Pro Tip:** Change `device` from `'cuda'` to `'cpu'` and re-run to see the performance difference!\n",
        "\n",
        "#### ðŸ—ï¸ **Model Selection**\n",
        "We'll use the **2-layer network** (`NN2Layer`) which is equivalent to our NumPy implementation:\n",
        "- Input size: 784 (28Ã—28 flattened)\n",
        "- Hidden layer: 32 neurons\n",
        "- Output size: 10 classes\n",
        "\n",
        "You can comment/uncomment to switch between 1-layer and 2-layer models.\n",
        "\n",
        "#### ðŸ“‰ **Loss Function**\n",
        "- **CrossEntropyLoss:** Combines softmax + negative log-likelihood\n",
        "- Ideal for multi-class classification\n",
        "- PyTorch's implementation is numerically stable\n",
        "\n",
        "#### ðŸ”§ **Optimizer**\n",
        "- **Adam:** Adaptive learning rate optimizer\n",
        "- Combines benefits of momentum and RMSProp\n",
        "- Generally works well with minimal tuning\n",
        "\n",
        "#### ðŸ“Š **Tracking**\n",
        "We maintain lists to store training and validation losses for visualization later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCMTZkue74nL",
        "outputId": "0033c29d-194b-477c-be0d-a21a2cd8ea7f"
      },
      "outputs": [],
      "source": [
        "num_epochs = 12\n",
        "lr = 1e-4\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # checks if machine supports cuda and if it does, we use that, otherwise cpu\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Choose model architecture\n",
        "# model = NN1Layer(28*28, 10)  # Single-layer: 28*28 input, 10 output classes (0-9)\n",
        "model = NN2Layer(28*28, 32, 10)  # Two-layer: equivalent to NumPy implementation\n",
        "\n",
        "# Configure optimizer and loss function\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()  # Multi-class classification loss\n",
        "\n",
        "# Move model to GPU/CPU\n",
        "model.to(device)  # All input tensors must also be moved to this device\n",
        "\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r4KeFeG8mF4"
      },
      "source": [
        "### ðŸ” **Pre-Training Performance Evaluation**\n",
        "\n",
        "Before training, let's evaluate the model's initial (random) performance on the test set.\n",
        "\n",
        "#### ðŸŽ¯ **Purpose**\n",
        "- Establish a **baseline** to compare against after training\n",
        "- Verify that the model and data pipeline work correctly\n",
        "- For random initialization, we expect ~10% accuracy (random guessing for 10 classes)\n",
        "\n",
        "#### âš™ï¸ **Evaluation Process**\n",
        "1. **Set model to evaluation mode:** `model.eval()`\n",
        "   - Disables dropout and batch normalization training behaviors\n",
        "   \n",
        "2. **Disable gradient computation:** `torch.no_grad()`\n",
        "   - Saves memory and speeds up inference\n",
        "   - Gradients aren't needed during evaluation\n",
        "\n",
        "3. **Compute metrics:**\n",
        "   - **Loss:** Average cross-entropy over all test samples\n",
        "   - **Accuracy:** Percentage of correctly classified images\n",
        "\n",
        "This gives us a clear picture of the untrained model's performance before any learning occurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-qDqF6f8oIp",
        "outputId": "ed76a846-1299-475d-fb8f-69517e69c9b3"
      },
      "outputs": [],
      "source": [
        "# Evaluate untrained model on test set\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "correctly_labelled = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient tracking for efficiency\n",
        "  \n",
        "  val_epoch_weighted_loss = 0\n",
        "  \n",
        "  for val_batch_X, val_batch_y in test_loader:\n",
        "    \n",
        "    # Move data to device and reshape\n",
        "    val_batch_X = val_batch_X.view(-1, 28*28).to(device)\n",
        "    val_batch_y = val_batch_y.to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    val_batch_y_probs = model(val_batch_X)\n",
        "    \n",
        "    # Calculate loss\n",
        "    loss = criterion(val_batch_y_probs, val_batch_y)\n",
        "    val_epoch_weighted_loss += (len(val_batch_y) * loss.item())\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    val_batch_y_pred = val_batch_y_probs.argmax(dim=1)  # Get class with highest probability\n",
        "    correctly_labelled += (val_batch_y_pred == val_batch_y).sum().item()\n",
        "\n",
        "  val_epoch_loss = val_epoch_weighted_loss / len(test_loader.dataset)\n",
        "\n",
        "print(f'Pre-training: val_loss={val_epoch_loss:.4f}, {correctly_labelled}/{len(test_loader.dataset)} correctly labelled ({correctly_labelled/len(test_loader.dataset)*100:.2f}% accuracy)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZzBFOA48Fb7"
      },
      "source": [
        "### ðŸš€ **Training Loop: Full Model Training with Validation**\n",
        "\n",
        "Now we train the neural network for multiple epochs using **batch gradient descent** with PyTorch's automatic differentiation.\n",
        "\n",
        "#### ðŸ”„ **Training Process (Per Epoch)**\n",
        "\n",
        "**ðŸ“š Training Phase:**\n",
        "1. **Set model to training mode:** `model.train()`\n",
        "   - Enables dropout, batch normalization, and other training-specific layers\n",
        "\n",
        "2. **For each batch:**\n",
        "   - Move data to device (GPU/CPU)\n",
        "   - Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784)\n",
        "   - **Forward pass:** Compute predictions\n",
        "   - **Loss computation:** Calculate cross-entropy loss\n",
        "   - **Backward pass:** `loss.backward()` â€” automatic gradient computation!\n",
        "   - **Optimization:** `optimizer.step()` â€” update weights\n",
        "   - **Gradient reset:** `optimizer.zero_grad()` â€” clear old gradients\n",
        "\n",
        "3. **Track training loss** across all batches\n",
        "\n",
        "**âœ… Validation Phase:**\n",
        "1. **Set model to evaluation mode:** `model.eval()`\n",
        "   \n",
        "2. **Disable gradient tracking:** `torch.no_grad()`\n",
        "   - Saves memory and speeds up evaluation\n",
        "\n",
        "3. **For each test batch:**\n",
        "   - Compute predictions and loss\n",
        "   - Calculate accuracy by comparing predicted vs true labels\n",
        "   - Track validation loss\n",
        "\n",
        "4. **Print epoch metrics:**\n",
        "   - Training loss, validation loss, and accuracy\n",
        "\n",
        "---\n",
        "\n",
        "#### âš¡ **GPU Acceleration**\n",
        "- The `%%time` magic command measures total training time\n",
        "- **Experiment:** Change `device` variable to `'cpu'` and re-run to see the speedup from GPU!\n",
        "\n",
        "---\n",
        "\n",
        "#### ðŸ“Š **Expected Behavior**\n",
        "- **Training loss** should decrease steadily\n",
        "- **Validation loss** should also decrease\n",
        "- **Accuracy** should improve (aim for >95%)\n",
        "- If validation loss increases while training loss decreases â†’ **overfitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy0ChPzH7_wx",
        "outputId": "3a5ba8e6-7a29-4c18-83fc-fcb7dcff67bf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for epoch_no in range(num_epochs):\n",
        "  \n",
        "  # ============ TRAINING PHASE ============\n",
        "  model.train()  # Enable training-specific layers (dropout, batch norm, etc.)\n",
        "  \n",
        "  epoch_weighted_loss = 0\n",
        "  \n",
        "  for batch_X, batch_y in train_loader:\n",
        "    \n",
        "    # Move data to device and reshape\n",
        "    batch_X = batch_X.view(-1, 28*28).to(device)  # Flatten: (batch_size, 1, 28, 28) â†’ (batch_size, 784)\n",
        "    batch_y = batch_y.to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    batch_y_probs = model(batch_X)  # Output: (batch_size, 10) â€” probabilities for each class\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = criterion(batch_y_probs, batch_y)\n",
        "    \n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Clear gradients from previous batch\n",
        "    loss.backward()        # Compute gradients via backpropagation\n",
        "    optimizer.step()       # Update weights using gradients\n",
        "    \n",
        "    # Track loss\n",
        "    epoch_weighted_loss += (len(batch_y) * loss.item())\n",
        "  \n",
        "  # Average training loss for epoch\n",
        "  epoch_loss = epoch_weighted_loss / len(train_loader.dataset)\n",
        "  train_losses.append(epoch_loss)\n",
        "  \n",
        "  \n",
        "  # ============ VALIDATION PHASE ============\n",
        "  model.eval()  # Disable training-specific layers\n",
        "  correctly_labelled = 0\n",
        "  \n",
        "  with torch.no_grad():  # Disable gradient tracking for efficiency\n",
        "    \n",
        "    val_epoch_weighted_loss = 0\n",
        "    \n",
        "    for val_batch_X, val_batch_y in test_loader:\n",
        "      \n",
        "      # Move data to device and reshape\n",
        "      val_batch_X = val_batch_X.view(-1, 28*28).to(device)\n",
        "      val_batch_y = val_batch_y.to(device)\n",
        "      \n",
        "      # Forward pass\n",
        "      val_batch_y_probs = model(val_batch_X)\n",
        "      \n",
        "      # Compute loss\n",
        "      loss = criterion(val_batch_y_probs, val_batch_y)\n",
        "      val_epoch_weighted_loss += (len(val_batch_y) * loss.item())\n",
        "      \n",
        "      # Calculate predictions and accuracy\n",
        "      val_batch_y_pred = val_batch_y_probs.argmax(dim=1)  # Convert probabilities to class labels\n",
        "      correctly_labelled += (val_batch_y_pred == val_batch_y).sum().item()\n",
        "    \n",
        "    # Average validation loss for epoch\n",
        "    val_epoch_loss = val_epoch_weighted_loss / len(test_loader.dataset)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "  \n",
        "  # Print epoch summary\n",
        "  print(f'Epoch {epoch_no+1}/{num_epochs}: train_loss={epoch_loss:.4f}, val_loss={val_epoch_loss:.4f}, accuracy={correctly_labelled}/{len(test_loader.dataset)} ({correctly_labelled/len(test_loader.dataset)*100:.2f}%)')\n",
        "\n",
        "print(f'\\nâœ… Training complete on device: {device}')\n",
        "print('ðŸ’¡ Tip: Change device variable to \"cpu\" and re-run to compare performance!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvNZwZsd_4Nf"
      },
      "source": [
        "### ðŸ“ˆ **Visualizing Training Progress: Loss Trajectory**\n",
        "\n",
        "After training, we plot the **training and validation loss curves** to understand the model's learning behavior.\n",
        "\n",
        "#### ðŸ” **Interpretation**\n",
        "\n",
        "**âœ… Good Signs:**\n",
        "- Both losses **decrease** over epochs\n",
        "- Curves are **relatively smooth**\n",
        "- Training and validation losses stay **close together**\n",
        "- Model has **converged** (losses plateau)\n",
        "\n",
        "**âš ï¸ Warning Signs:**\n",
        "- **Validation loss increases** while training loss decreases â†’ **Overfitting**\n",
        "  - Model memorizes training data but doesn't generalize\n",
        "  - Solutions: Add regularization, dropout, or early stopping\n",
        "  \n",
        "- **Both losses remain high** â†’ **Underfitting**\n",
        "  - Model too simple or learning rate too small\n",
        "  - Solutions: Increase model capacity, adjust learning rate\n",
        "\n",
        "- **Noisy/erratic curves** â†’ Training instability\n",
        "  - Solutions: Reduce learning rate, increase batch size\n",
        "\n",
        "This visualization is crucial for diagnosing training issues and understanding model performance!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Jtaq-nooqSj9",
        "outputId": "d30f6cf3-53fe-4cc2-e295-04dc2128f2d6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss', linewidth=2, marker='o')\n",
        "plt.plot(val_losses, label='Validation Loss', linewidth=2, marker='s')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (Cross Entropy)', fontsize=12)\n",
        "plt.title('Training Curve â€” MNIST Digit Classification (PyTorch)', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Jf5i7IAswc"
      },
      "source": [
        "### ðŸ”¢ **Understanding Tensor Shapes and Softmax Behavior**\n",
        "\n",
        "Let's examine the shapes of our tensors and verify that softmax works as expected.\n",
        "\n",
        "#### ðŸ“ **Tensor Dimensions**\n",
        "- **Input:** `(batch_size, 784)` â€” flattened images\n",
        "- **Predictions (probabilities):** `(batch_size, 10)` â€” probability for each of 10 classes\n",
        "- **Labels (predicted):** `(batch_size,)` â€” single class index per sample\n",
        "\n",
        "#### ðŸ” **Verifying Softmax**\n",
        "Softmax should convert logits into a **probability distribution**:\n",
        "- Each row sums to **1.0**\n",
        "- Each value is between **0.0 and 1.0**\n",
        "- Highest logit becomes highest probability\n",
        "\n",
        "#### ðŸŽ¯ **Argmax in Action**\n",
        "`argmax(dim=1)` finds the **index** of the maximum value:\n",
        "- Input: `[0.01, 0.05, 0.02, 0.87, 0.01, ...]` (probabilities)\n",
        "- Output: `3` (index of 0.87)\n",
        "\n",
        "This converts probability distributions into concrete class predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOaxXyMg-ahW",
        "outputId": "ed1b3391-2211-4623-f28a-fbd71f91e563"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"TENSOR SHAPES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"Input shape (flattened images): {val_batch_X.shape}\")\n",
        "print(f\"True labels shape: {val_batch_y.shape}\")\n",
        "print(f\"Predicted probabilities shape: {val_batch_y_probs.shape}\")\n",
        "print(f\"Predicted labels shape: {val_batch_y_pred.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"VERIFYING SOFTMAX (probabilities should sum to 1)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Sum probabilities across classes (dim=1)\n",
        "prob_sums = val_batch_y_probs.sum(1)\n",
        "print(f\"\\nSum of probabilities for each sample:\\n{prob_sums}\")\n",
        "print(f\"\\nShape of sums: {prob_sums.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ARGMAX IN ACTION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "test_idx = 15\n",
        "print(f\"\\nSample {test_idx} probabilities (10 classes):\")\n",
        "print(val_batch_y_probs[test_idx].cpu())  # .cpu() moves tensor from GPU to CPU for display\n",
        "\n",
        "print(f\"\\nPredicted class (argmax):\")\n",
        "print(val_batch_y_pred[test_idx].cpu())\n",
        "\n",
        "print(f\"\\nTrue class:\")\n",
        "print(val_batch_y[test_idx].cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iw2G2akA5lB"
      },
      "source": [
        "### ðŸ’¾ **Saving and Loading Trained Models**\n",
        "\n",
        "PyTorch makes it easy to save trained models and load them later for inference or continued training.\n",
        "\n",
        "#### ðŸ“¦ **Saving a Model**\n",
        "```python\n",
        "torch.save(model.state_dict(), 'model_name.pt')\n",
        "```\n",
        "- **`state_dict()`** contains all model parameters (weights and biases)\n",
        "- Saved as a dictionary mapping parameter names to tensors\n",
        "- File extension `.pt` or `.pth` is standard convention\n",
        "\n",
        "#### ðŸ“‚ **Loading a Model**\n",
        "```python\n",
        "model = ModelClass(...)          # Create model with same architecture\n",
        "model.load_state_dict(torch.load('model_name.pt'))\n",
        "model.eval()                     # Set to evaluation mode\n",
        "```\n",
        "\n",
        "#### âš ï¸ **Important Notes**\n",
        "- Must create model with **identical architecture** before loading\n",
        "- Use `model.eval()` after loading for inference\n",
        "- Move model to correct device: `model.to(device)`\n",
        "\n",
        "#### ðŸ§ª **Verification**\n",
        "We'll demonstrate by:\n",
        "1. Creating a fresh (untrained) model â†’ poor performance\n",
        "2. Loading saved weights â†’ excellent performance\n",
        "\n",
        "This proves the weights were successfully saved and restored!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi61n3DJAtvp"
      },
      "outputs": [],
      "source": [
        "# Save the trained model's state dictionary\n",
        "torch.save(model.state_dict(), 'MNIST_classifier.pt')\n",
        "\n",
        "print(\"âœ… Model saved successfully as 'MNIST_classifier.pt'\")\n",
        "print(\"ðŸ“ Check your storage/file system to see the saved file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTWzqt7SCQaz"
      },
      "source": [
        "### ðŸ”„ **Demonstrating Model Loading**\n",
        "\n",
        "Let's verify that our saved model can be loaded and produces the same results.\n",
        "\n",
        "#### ðŸ§ª **Experiment Design**\n",
        "1. **Create untrained model** with random weights\n",
        "   - Expected: Poor performance (~10% accuracy)\n",
        "\n",
        "2. **Load trained weights** from saved file\n",
        "   - Expected: Excellent performance (>95% accuracy)\n",
        "\n",
        "This demonstrates:\n",
        "- Models start with random initialization\n",
        "- Saved weights perfectly restore trained performance\n",
        "- Model persistence works correctly\n",
        "\n",
        "#### ðŸ’¡ **Practical Use Cases**\n",
        "- **Deployment:** Load trained model for production inference\n",
        "- **Transfer Learning:** Load pre-trained weights and fine-tune\n",
        "- **Checkpointing:** Save during training to resume later\n",
        "- **Sharing:** Distribute trained models to others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n4cITtHB6Xa"
      },
      "outputs": [],
      "source": [
        "# Create a fresh model with random initialization\n",
        "loaded_model = NN2Layer(28*28, 32, 10)\n",
        "loaded_model.eval().to(device)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BEFORE LOADING TRAINED WEIGHTS (Random Initialization)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with torch.no_grad():\n",
        "  probs = loaded_model(val_batch_X)\n",
        "  preds = probs.argmax(dim=1)\n",
        "  \n",
        "  correct = (preds == val_batch_y).sum()\n",
        "  total = len(preds)\n",
        "  accuracy = (correct / total * 100).item()\n",
        "  \n",
        "  print(f\"Accuracy: {correct}/{total} correct ({accuracy:.2f}%)\")\n",
        "  print(\"Expected: ~10% (random guessing for 10 classes)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"AFTER LOADING TRAINED WEIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load the saved weights\n",
        "loaded_model.load_state_dict(torch.load('MNIST_classifier.pt'))\n",
        "\n",
        "with torch.no_grad():\n",
        "  probs = loaded_model(val_batch_X)\n",
        "  preds = probs.argmax(dim=1)\n",
        "  \n",
        "  correct = (preds == val_batch_y).sum()\n",
        "  total = len(preds)\n",
        "  accuracy = (correct / total * 100).item()\n",
        "  \n",
        "  print(f\"Accuracy: {correct}/{total} correct ({accuracy:.2f}%)\")\n",
        "  print(\"Expected: >95% (trained model performance)\")\n",
        "\n",
        "print(\"\\nâœ… Model successfully loaded and verified!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§¾ **Summary**\n",
        "\n",
        "In this experiment, we successfully built and trained neural networks for MNIST digit classification using PyTorch.\n",
        "\n",
        "#### ðŸŽ¯ **What We Accomplished**\n",
        "\n",
        "1. **Loaded and preprocessed** the MNIST dataset using `torchvision` and `DataLoader`\n",
        "\n",
        "2. **Implemented two neural network architectures:**\n",
        "   - **1-Layer Network:** Direct input-to-output mapping\n",
        "   - **2-Layer Network:** With hidden layer and ReLU activation\n",
        "\n",
        "3. **Trained models** using:\n",
        "   - **Automatic differentiation** (no manual gradient computation!)\n",
        "   - **Adam optimizer** for efficient weight updates\n",
        "   - **Cross-entropy loss** for multi-class classification\n",
        "\n",
        "4. **Leveraged GPU acceleration** for significantly faster training\n",
        "\n",
        "5. **Visualized training progress** through loss curves\n",
        "\n",
        "6. **Saved and loaded models** for persistence and deployment\n",
        "\n",
        "---\n",
        "\n",
        "#### ðŸ”‘ **Key Takeaways**\n",
        "\n",
        "âœ… **PyTorch Advantages:**\n",
        "- **Automatic differentiation** eliminates manual backpropagation\n",
        "- **GPU support** dramatically speeds up training\n",
        "- **Modular design** with `nn.Module` creates reusable components\n",
        "- **Rich ecosystem** with pre-built layers, optimizers, and utilities\n",
        "\n",
        "âœ… **Best Practices:**\n",
        "- Always move model and data to the same device (`cuda` or `cpu`)\n",
        "- Use `model.train()` during training and `model.eval()` during evaluation\n",
        "- Track both training and validation losses to detect overfitting\n",
        "- Save models regularly for checkpointing and deployment\n",
        "\n",
        "âœ… **Performance:**\n",
        "- Achieved **>95% accuracy** on MNIST with simple 2-layer network\n",
        "- GPU training is **WAY FASTER** than CPU (depending on hardware)\n",
        "\n",
        "\n",
        "#### ðŸ“š **Comparison: NumPy vs PyTorch**\n",
        "\n",
        "| Aspect | NumPy Implementation | PyTorch Implementation |\n",
        "|--------|---------------------|----------------------|\n",
        "| **Gradients** | Manual computation | Automatic (autograd) |\n",
        "| **Speed** | CPU only | GPU accelerated |\n",
        "| **Code Length** | More verbose | More concise |\n",
        "| **Flexibility** | Full control | High-level abstractions |\n",
        "| **Learning Value** | Understand fundamentals | Production-ready code |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
